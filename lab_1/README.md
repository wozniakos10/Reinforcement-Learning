This section stores the implementation and comparison of different algorithms for the k-armed bandit problem under various scenarios:
stationary, non-stationary, constant initial reward, and random initial reward.

The implemented algorithms are:

- Explore-then-commit
- Epsilon-Greedy
- Upper Confidence Bound
- Gradient Bandit
- Thompson Sampling